{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-27T12:26:48.165272Z",
     "start_time": "2024-08-27T12:26:48.153272Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T12:26:49.962917Z",
     "start_time": "2024-08-27T12:26:48.168276Z"
    }
   },
   "id": "3384d3490c1c8a38",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "735c9cb7be2445d888a56f89649dd13f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denlu\\PycharmProjects\\grup_project\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\denlu\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aec86d5be2d54e6f84b330f49774a42f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42d32db4378d4858a3a17d13ee5c2e98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94fe17aae2d946c2845704d71602fdf8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denlu\\PycharmProjects\\grup_project\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Ініціалізація токенізатора BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T12:26:52.839459Z",
     "start_time": "2024-08-27T12:26:49.963910Z"
    }
   },
   "id": "e4ffd4cb58f7203e",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Функція очищення тексту\n",
    "def clean_text(text):\n",
    "    # Видалення HTML-тегів\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Видалення спеціальних символів і цифр\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Перетворення тексту у нижній регістр\n",
    "    text = text.lower()\n",
    "    # Видалення зайвих пробілів\n",
    "    text = text.strip()\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T12:26:52.854468Z",
     "start_time": "2024-08-27T12:26:52.842459Z"
    }
   },
   "id": "54d7294793f68e50",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Застосування функції очищення до тренувального набору даних\n",
    "train_data['cleaned_comment_text'] = train_data['comment_text'].apply(clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T12:26:54.284462Z",
     "start_time": "2024-08-27T12:26:52.857458Z"
    }
   },
   "id": "483962440f38847",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Перетворення тексту на токени, використовуючи BERT токенізатор\n",
    "train_encodings = tokenizer(train_data['cleaned_comment_text'].tolist(), truncation=True, padding=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T12:29:50.625145Z",
     "start_time": "2024-08-27T12:26:54.287466Z"
    }
   },
   "id": "1e2cfdfede51b35c",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        comment_text  \\\n",
      "0  Explanation\\nWhy the edits made under my usern...   \n",
      "1  D'aww! He matches this background colour I'm s...   \n",
      "2  Hey man, I'm really not trying to edit war. It...   \n",
      "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
      "4  You, sir, are my hero. Any chance you remember...   \n",
      "\n",
      "                                cleaned_comment_text  \n",
      "0  explanation\\nwhy the edits made under my usern...  \n",
      "1  daww he matches this background colour im seem...  \n",
      "2  hey man im really not trying to edit war its j...  \n",
      "3  more\\ni cant make any real suggestions on impr...  \n",
      "4  you sir are my hero any chance you remember wh...  \n"
     ]
    }
   ],
   "source": [
    "# Перегляд перших 5 рядків очищених даних\n",
    "print(train_data[['comment_text', 'cleaned_comment_text']].head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T12:29:50.673153Z",
     "start_time": "2024-08-27T12:29:50.626178Z"
    }
   },
   "id": "e28523d765129310",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Приклад токенізованого тексту: [101, 7526, 2339, 1996, 10086, 2015, 2081, 2104, 2026, 5310, 18442, 13076, 12392, 2050, 5470, 2020, 16407, 2027, 4694, 2102, 3158, 9305, 22556, 2074, 8503, 2006, 2070, 3806, 2044, 1045, 5444, 2012, 2047, 2259, 14421, 6904, 2278, 1998, 3531, 2123, 2102, 6366, 1996, 23561, 2013, 1996, 2831, 3931, 2144, 10047, 3394, 2085, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Перегляд інформації про токенізовані дані\n",
    "print(\"Приклад токенізованого тексту:\", train_encodings['input_ids'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T12:29:50.689150Z",
     "start_time": "2024-08-27T12:29:50.675147Z"
    }
   },
   "id": "b60621f8847a666a",
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
